1. Problem Definition (6 points)
Example Problem:

Predicting Student Dropout Rates in Higher Education

Three Objectives:

Identify at-risk students early (by end of first semester)
Achieve 85% prediction accuracy for dropout likelihood
Provide actionable insights for intervention strategies
Two Stakeholders:

Academic advisors and student support services
University administration and policy makers
Key Performance Indicator (KPI):

F1-Score of dropout prediction (balances precision and recall), targeting at least 0.80 to ensure we identify at-risk students while minimizing false alarms

2. Data Collection & Preprocessing (8 points)
Two Data Sources:

Student Information System (SIS): academic records, grades, attendance
Learning Management System (LMS): engagement metrics, assignment submissions
One Potential Bias:

Selection Bias: Historical data may over-represent certain demographics or socioeconomic groups who had different access to support services, leading to biased predictions that disadvantage underrepresented groups.

Three Preprocessing Steps:

Handle Missing Data: Use multiple imputation for missing grades; drop records with more than 30% missing features
Feature Scaling: Normalize GPA (0-4 scale) and standardize engagement metrics (z-score normalization)
Encode Categorical Variables: One-hot encode major/department; label encode ordinal variables like year level
3. Model Development (8 points)
Model Choice: Gradient Boosting (XGBoost)

Justification: XGBoost handles mixed data types well, provides feature importance for interpretability, manages imbalanced classes effectively, and has proven performance on tabular educational data.

Data Splitting Strategy:

Training Set: 70% - for model learning
Validation Set: 15% - for hyperparameter tuning and early stopping
Test Set: 15% - for final unbiased performance evaluation
Use stratified sampling to maintain dropout rate proportions across all sets
Two Hyperparameters to Tune:

learning_rate: Controls step size in gradient descent; lower values (0.01-0.1) prevent overfitting and improve generalization
max_depth: Limits tree complexity; tuning (3-10) balances model capacity with overfitting prevention
4. Evaluation & Deployment (8 points)
Two Evaluation Metrics:

F1-Score: Balances precision and recall, crucial for imbalanced dropout data where both false positives and false negatives have costs
AUC-ROC: Measures discrimination ability across all classification thresholds, helping optimize intervention targeting
Concept Drift:

Concept drift occurs when the statistical properties of the target variable change over time, making the model less accurate.

Monitoring Strategy:

Track prediction performance metrics monthly
Compare feature distributions between training and production data
Set up alerts when F1-score drops below 0.75
Retrain quarterly or when drift is detected
Deployment Challenge:

Real-time Integration: Synchronizing predictions with multiple data sources (SIS, LMS) that update asynchronously presents latency and consistency challenges. Solution involves implementing a data pipeline with scheduled batch processing and caching mechanisms

Part 2

1. Problem Scope (5 points)
Problem Definition:

Develop a predictive model to identify patients at high risk of unplanned hospital readmission within 30 days post-discharge, enabling proactive intervention and resource allocation.

Objectives:

Achieve 80% sensitivity in identifying high-risk patients
Reduce 30-day readmission rates by 15% within 12 months
Provide interpretable risk scores for clinical decision support
Stakeholders:

Hospital administrators (cost reduction, quality metrics)
Clinical staff (physicians, nurses, discharge planners)
Patients (improved care, reduced complications)
Insurance providers (reimbursement implications)
2. Data Strategy (10 points)
Data Sources:

Electronic Health Records (EHR): diagnosis codes, medications, procedures, lab results
Patient demographics: age, gender, socioeconomic indicators, insurance status
Administrative data: length of stay, discharge disposition, previous admissions
Clinical notes: discharge summaries (using NLP for extraction)
Two Ethical Concerns:

Patient Privacy (HIPAA Compliance): PHI must be de-identified or encrypted; access controls required; audit trails for all data access
Algorithmic Fairness: Model may perpetuate healthcare disparities if training data reflects historical biases against minority groups or low-income patients
Preprocessing Pipeline:

Step 1: Data Cleaning

Remove duplicate records and impossible values
Handle missing data: mean imputation for vitals, mode for categorical
Step 2: Feature Engineering

Create Charlson Comorbidity Index from diagnosis codes
Calculate medication count and polypharmacy flag
Engineer time-based features (days since last admission)
Extract sentiment from clinical notes
Step 3: Normalization & Encoding

StandardScaler for continuous variables (age, lab values)
One-hot encode diagnosis categories and discharge types
Create interaction features (age times comorbidity score)
3. Model Development (10 points)
Model Selection: Logistic Regression with Regularization

Justification:

Highly interpretable - provides odds ratios for clinical understanding
Fast inference suitable for real-time clinical workflows
Well-studied in healthcare with regulatory acceptance
L2 regularization prevents overfitting on correlated features
Outputs calibrated probabilities for risk stratification
Confusion Matrix (Hypothetical Data):

Predicted: No Readmit	Predicted: Readmit
Actual: No Readmit	850 (TN)	50 (FP)
Actual: Readmit	30 (FN)	70 (TP)
Calculations:

Precision = TP / (TP + FP) = 70 / (70 + 50) = 0.583 (58.3%)

Recall (Sensitivity) = TP / (TP + FN) = 70 / (70 + 30) = 0.700 (70.0%)

Interpretation: The model identifies 70% of actual readmissions but has moderate precision, generating some false alarms.

4. Deployment (10 points)
Integration Steps:

API Development: Create RESTful API endpoint for model predictions
EHR Integration: Connect API to hospital's Epic/Cerner system via HL7/FHIR
Batch Processing: Implement nightly scoring of all discharge patients
Dashboard Development: Build clinician interface displaying risk scores
Alert System: Notify care coordinators of high-risk patients (over 70% risk)
Monitoring: Deploy logging and performance tracking infrastructure
HIPAA Compliance Strategy:

Data Encryption: AES-256 for data at rest, TLS 1.3 for transmission
Access Controls: Role-based authentication, multi-factor for API access
Audit Trails: Log all PHI access with timestamps and user IDs
Business Associate Agreements: With all cloud vendors
De-identification: Use safe harbor method for analytics/research
Minimum Necessary: Restrict data access to required fields only
Regular Risk Assessments: Annual HIPAA security audits
5. Optimization (5 points)
Method to Address Overfitting: Cross-Validation with Regularization

Strategy: Implement 5-fold stratified cross-validation combined with L2 regularization (Ridge regression).

How it works:

Cross-validation ensures model generalizes across different data subsets
L2 penalty (lambda parameter) shrinks less important feature coefficients
Grid search to find optimal lambda (test range: 0.001 to 100)
Monitor training vs. validation performance gap
Early stopping if validation loss plateaus
Additional techniques: Feature selection using LASSO, collecting more diverse training data, ensemble methods to reduce variance

Part 3: Critical Thinking
Ethics & Bias (10 points)
How Biased Training Data Affects Patient Outcomes:

1. Disparate Treatment Across Demographics

If training data contains fewer examples from minority populations or underserved communities, the model may underperform for these groups. For example, if African American patients are underrepresented, the model might fail to accurately predict their readmission risk, leading to inadequate preventive care and perpetuating health disparities.

2. Socioeconomic Bias

Historical data may reflect systemic inequalities where low-income patients had less access to follow-up care or medications. The model might incorrectly learn that socioeconomic status predicts readmission when it actually reflects lack of resources, leading to self-fulfilling prophecies where these patients receive less intervention.

3. Feedback Loop Amplification

If the model is biased against certain groups, those patients may receive fewer interventions, leading to worse outcomes, which then reinforces the bias in future training data—creating a harmful cycle of discrimination.

4. Clinical Decision Impact

Clinicians relying on biased predictions might allocate resources (home health visits, extended observation, medication assistance) unfairly, denying critical support to vulnerable populations most in need.

Strategy to Mitigate Bias:

Fairness-Aware Model Training with Subgroup Performance Monitoring

Implementation:

Stratified Evaluation: Measure model performance (precision, recall, F1) separately for protected demographic groups (race, gender, age, income level)
Fairness Constraints: Use fairness-aware algorithms (e.g., fairness constraints in optimization) to ensure similar false negative rates across groups—critical since false negatives deny needed care
Balanced Training: Oversample underrepresented groups or use synthetic data generation (SMOTE) to ensure adequate representation
Adversarial Debiasing: Train a secondary model to predict sensitive attributes; penalize the primary model if it relies on these attributes
Regular Audits: Conduct quarterly fairness audits comparing outcomes across patient populations, with transparency reports to clinical ethics committees
Clinical Review: Implement human-in-the-loop review for borderline cases, ensuring clinical expertise catches potential bias
This multi-pronged approach addresses bias at data, algorithm, and deployment levels while maintaining clinical oversight.

Trade-offs (10 points)
Model Interpretability vs. Accuracy in Healthcare:

The Core Trade-off:

Complex models like deep neural networks or ensemble methods (Random Forests, XGBoost) often achieve higher predictive accuracy but operate as black boxes, making it difficult to explain individual predictions. Simpler models like logistic regression are highly interpretable (showing exact feature contributions) but may have lower accuracy on complex patterns.

Why Interpretability Matters in Healthcare:

Clinical Trust: Physicians need to understand why a patient is flagged as high-risk to make informed decisions and override when appropriate
Regulatory Compliance: Healthcare regulations often require explainable decisions for liability and patient rights
Bias Detection: Interpretable models make it easier to identify if the model is using inappropriate features (e.g., race as a direct predictor)
Patient Communication: Clinicians must explain risk factors to patients for shared decision-making
Balancing the Trade-off:

Hybrid Approach: Use ensemble models for prediction accuracy but add post-hoc interpretability tools (SHAP values, LIME) to explain individual predictions
Layered Decision Support: Simple rule-based models for initial triage, complex models for detailed risk scoring, with interpretable summaries
Acceptable Accuracy Loss: In healthcare, a 2-3% accuracy reduction for full interpretability may be worthwhile to maintain clinical trust and regulatory compliance
Context-Dependent: For low-stakes screening, prioritize accuracy; for high-stakes treatment decisions, prioritize interpretability
Recommendation for This Case Study:

Use logistic regression as the primary model for its interpretability, supplemented by an XGBoost model for performance comparison. If accuracy gap is less than 5%, favor logistic regression. Deploy SHAP values regardless of final model choice to provide feature-level explanations to clinicians.

Impact of Limited Computational Resources:

Resource Constraints Affect Multiple Dimensions:

Training Time: Complex models (deep learning, extensive hyperparameter tuning) require significant compute time that may be infeasible
Inference Speed: Real-time clinical workflows require fast predictions; computationally expensive models create unacceptable delays
Storage: Large models and extensive training data require substantial storage infrastructure
Scalability: Hospital-wide deployment serving thousands of predictions daily needs efficient architecture
Model Choice Adaptations:

Preferred Models:

Logistic Regression: Minimal compute, fast inference, small model size (around KB)
Decision Trees: Low complexity, no GPU needed, interpretable
Naive Bayes: Extremely fast training and prediction
Avoid:

Deep Neural Networks: Require GPUs, extensive training time
Large Ensemble Methods: Memory-intensive, slower inference
Complex hyperparameter searches: Grid search over large parameter spaces
Optimization Strategies:

Feature Selection: Reduce dimensionality using correlation analysis, keeping only top 15-20 predictive features
Efficient Algorithms: Use sparse matrix representations, mini-batch gradient descent
Cloud Deployment: Leverage serverless functions (AWS Lambda) for cost-effective scaling
Model Compression: Prune unnecessary parameters, use quantization to reduce model size
Batch Processing: Score patients overnight rather than real-time to spread computational load
Caching: Store predictions for static features, only recompute when data changes
Practical Impact on This Case Study:

With limited resources, I would choose regularized logistic regression over XGBoost. It provides acceptable accuracy (typically 75-82% for readmission prediction), trains in minutes on CPU, requires minimal storage, and achieves sub-second inference. The model can run on basic hospital servers without specialized hardware. This ensures sustainable deployment within budgetary constraints while maintaining clinical utility.

Part 4:
Reflection (5 points)
Most Challenging Part of the Workflow:

Balancing Ethical Considerations with Technical Performance

The most challenging aspect was navigating the tension between model accuracy and fairness. While optimizing for predictive performance, I had to constantly consider how the model might perpetuate healthcare disparities. Specifically:

Ensuring representative training data while respecting patient privacy made data collection complex
Implementing fairness constraints without significantly degrading overall model performance required careful tuning
Determining which features to include was ethically fraught—socioeconomic indicators are predictive but could encode bias
Communicating uncertainty to clinicians without undermining trust in the system
Why it was challenging: Unlike pure technical problems with clear optimization objectives, ethical considerations require judgment calls with no correct answer. Healthcare AI operates in a domain where mistakes directly harm vulnerable people, making every design decision carry moral weight. The workflow taught me that responsible AI development requires interdisciplinary collaboration—data scientists cannot make these decisions alone.

How to Improve with More Time/Resources:

1. Enhanced Data Collection & Integration

Incorporate social determinants of health data (housing stability, food security, transportation access)
Integrate pharmacy data to track medication adherence patterns
Include post-discharge follow-up data (telehealth visits, patient portal engagement)
Partner with community health centers for more diverse patient representation
2. Advanced Modeling Techniques

Develop ensemble models combining multiple algorithms with meta-learning
Implement temporal models (LSTM networks) to capture patient trajectory over time
Use causal inference methods to identify modifiable risk factors vs. mere correlations
Build separate specialized models for different disease categories (cardiac, respiratory, etc.)
3. Comprehensive Fairness Evaluation

Conduct prospective bias audits across multiple protected attributes
Partner with medical ethicists and community representatives for fairness assessment
Implement multiple fairness metrics (demographic parity, equalized odds, calibration across groups)
Create transparency reports detailing model performance by subgroup
4. Clinical Validation & User Research

Run randomized controlled trial comparing outcomes with vs. without AI predictions
Conduct user experience studies with clinicians to optimize interface design
Implement A/B testing for different risk presentation formats
Gather qualitative feedback through clinician interviews and patient focus groups
5. Robust Deployment Infrastructure

Build comprehensive monitoring dashboards tracking prediction accuracy, data drift, and system usage
Develop automated retraining pipelines with human validation checkpoints
Implement sophisticated alerting system for both high-risk patients and model anomalies
Create detailed documentation and training materials for clinical staff
Most Critical Improvement:

If I could only choose one area for improvement, it would be prospective clinical validation through a randomized trial. This would provide definitive evidence of real-world impact and surface unforeseen issues that retrospective analysis cannot capture. It would also build clinician trust and inform iterative improvements based on actual usage patterns.

AI Development Workflow Diagram (5 points)
1. Problem Definition
Define objectives, stakeholders, KPIs
2. Data Collection
Gather data from multiple sources
3. Data Preprocessing
Clean, normalize, feature engineering
Handle missing data, encode variables
4. Exploratory Data Analysis
Visualize patterns, correlations, outliers
5. Model Selection
Choose algorithm, split data
6. Model Training
Train on data, tune hyperparameters
Cross-validation, regularization
7. Model Evaluation
Test metrics, confusion matrix, ROC
Performance
Satisfactory?
No
Iterate:
Adjust model
or features
Yes
8. Deployment
Integrate, monitor, maintain
9. Monitoring & Maintenance
Track performance, detect drift
Retrain as needed
Continuous
Improvement
Loop
Key Workflow Characteristics:

Iterative Process: Red No path shows returning to earlier stages when performance is unsatisfactory
Continuous Loop: Purple dashed line represents ongoing monitoring feeding back to data collection
Sequential Stages: Each stage builds on previous work, though iteration is expected
Decision Points: Yellow diamond represents critical evaluation checkpoint
Based on CRISP-DM Framework: Cross-Industry Standard Process for Data Mining